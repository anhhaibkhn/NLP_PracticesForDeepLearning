{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "the cities i like the most in viet nam is bac ninh, ha noi and da nang\n"
    }
   ],
   "source": [
    "# Exercise 1: Lowercasing/Uppercasing\n",
    "s = \"the cities I like the most in Viet nam is Bac Ninh, Ha Noi and Da Nang\"\n",
    "s = s.lower()\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['hanoi', 'hanoi', 'hanoi', 'hanooiii']\n"
    }
   ],
   "source": [
    "# Create an array and apply lower() for each element of words array then print \n",
    "words = ['Hanoi', 'hANoi', 'hANOI', 'HANOOIII']\n",
    "words = [word.lower() for word in words]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['sleepy', 'sleepy', 'sleepy', 'sleepy', 'sleepy', 'sleepy']\n"
    }
   ],
   "source": [
    "#Exercise 2: Removing Noise from Words \n",
    "# import the regular expression lib\n",
    "import re\n",
    "\n",
    "#create function call 'clean_words' to remove different types of noise form the words\n",
    "def clean_words(text):\n",
    "    # remove html markup\n",
    "    text = re.sub(\"(<.*?>)\",\"\",text)\n",
    "    # remove non-ascii and digits\n",
    "    text = re.sub(\"(\\W|_d+)\",\" \",text)\n",
    "    # remove whitespace \n",
    "    text = text.strip()\n",
    "    return text \n",
    "\n",
    "# Create new array of raw words with noise \n",
    "raw = ['...sleepy', 'sleepy!!', '#sleepy', '>>>>>>>>sleepy>>>>>>', '<a>sleepy</a>', '<a>>>>>>sleepy</a><<<,']\n",
    "\n",
    "# Apply clean_words\n",
    "clean = [clean_words(r) for r in raw]\n",
    "print(clean) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw word</th>\n      <th>stem</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>annoying</td>\n      <td>annoy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>annoyed</td>\n      <td>annoy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>annoys</td>\n      <td>annoy</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>annoy</td>\n      <td>annoy</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   raw word   stem\n0  annoying  annoy\n1   annoyed  annoy\n2    annoys  annoy\n3     annoy  annoy"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3: Stemming on Words\n",
    "# Text Normalization generally to fix incorrect spellings ex. Raw form: brb -> Canonical form: be right back\n",
    "# Stemming. Annoying -> Annoy\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer as ps\n",
    "\n",
    "# create instance stemmer\n",
    "stemmer = ps()\n",
    "words = ['annoying','annoyed','annoys','annoy']\n",
    "# apply stemmer for each word and put to new array stem\n",
    "stems = [stemmer.stem(word = word) for word in words]\n",
    "# print raw words and stems into DataFrame\n",
    "sdf = pd.DataFrame({'raw word': words, 'stem' : stems})\n",
    "sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>raw words</th>\n      <th>letimazed</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>troubling</td>\n      <td>trouble</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>troubles</td>\n      <td>trouble</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>troublesome</td>\n      <td>troublesome</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>troubled</td>\n      <td>trouble</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>trouble</td>\n      <td>trouble</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     raw words    letimazed\n0    troubling      trouble\n1     troubles      trouble\n2  troublesome  troublesome\n3     troubled      trouble\n4      trouble      trouble"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 4: Lemmatization on Words\n",
    "#import WordNetLemmatizer and download Wordnet\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "nltk.download('wordnet')\n",
    "# create an instance of lemmtizer\n",
    "lematizer = wnl()\n",
    "# create an array of different forms \n",
    "words = ['troubling','troubles','troublesome','troubled','trouble']\n",
    "# apply lemmatizer to each words , put to new array, \n",
    "# v denotes verb in \"pos\"\n",
    "lematized = [lematizer.lemmatize(word = word, pos = 'v') for word in words]\n",
    "#print\n",
    "ldf = pd.DataFrame({'raw words' : words, 'letimazed' : lematized})\n",
    "\n",
    "#ldf = ldf[['raw word','lemmatized']]\n",
    "ldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
    },
    {
     "data": {
      "text/plain": "['Hi', '!', 'My', 'name', 'is', 'Nguyen', 'Hai', '.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 5: Tokennizing words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize \n",
    "\n",
    "s = \"Hi! My name is Nguyen Hai.\"\n",
    "tokens = word_tokenize(s)\n",
    "\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['Wat sup.', 'My man.', 'How are you doing?.']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 6: Tokenizing Sentences\n",
    "from nltk import sent_tokenize\n",
    "s = \"Wat sup. My man. How are you doing?.\"\n",
    "tokens = sent_tokenize(s)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['the', 'weather', 'is', 'really', 'cold', 'but', 'I', 'still', 'want', 'to', 'go', 'for', 'a', 'swim']\n['cold', 'I', 'want', 'to', 'go', 'swim']\n['the', 'weather', 'is', 'really', 'but', 'still', 'for', 'a']\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
    }
   ],
   "source": [
    "# Additional BeautifulSoup for stripping HTML markup\n",
    "# Removing noisy words from corpus like the, this, and, it. \n",
    "\n",
    "# Exercise 7: Removing Stop Words\n",
    "\n",
    "nltk.download('stopwords')\n",
    "s = \"the weather is really cold but I still want to go for a swim\"\n",
    "# imort stopwords and create a set of English stop words as\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stopwords2 = ['the','weather','is','really','but','still','for','a']\n",
    "\n",
    "#Tokenize the sentence using word_tokenize , store those tokens that do not occur in stop_words in array\n",
    "tokens = word_tokenize(s)\n",
    "print(tokens)\n",
    "tokens = [word for word in tokens if  word not in stopwords2]\n",
    "print(tokens)\n",
    "print(stopwords2)\n",
    "# Possiblly to convert numbers to word forms. add noise removal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "thirty-six\nthirty-sixth\n36th\nthirty-six\nzero euro, thirty-six cents\nba mươi sáu\n"
    }
   ],
   "source": [
    "# Addtional Num to word\n",
    "\n",
    "from num2words import num2words \n",
    "  \n",
    "# Most common usage. \n",
    "print(num2words(36)) \n",
    "  \n",
    "# Other variants, according to the type of article. \n",
    "print(num2words(36, to = 'ordinal')) \n",
    "print(num2words(36, to = 'ordinal_num')) \n",
    "print(num2words(36, to = 'year')) \n",
    "print(num2words(36, to = 'currency')) \n",
    "  \n",
    "# Language Support. \n",
    "print(num2words(36, lang ='vi')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 12:51:32,177 : INFO : collecting all words and their counts\n2019-12-25 12:51:32,178 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n2019-12-25 12:51:32,179 : INFO : collected 17 word types from a corpus of 36 raw words and 6 sentences\n2019-12-25 12:51:32,179 : INFO : Loading a fresh vocabulary\n2019-12-25 12:51:32,180 : INFO : effective_min_count=1 retains 17 unique words (100% of original 17, drops 0)\n2019-12-25 12:51:32,180 : INFO : effective_min_count=1 leaves 36 word corpus (100% of original 36, drops 0)\n2019-12-25 12:51:32,181 : INFO : deleting the raw counts dictionary of 17 items\n2019-12-25 12:51:32,181 : INFO : sample=0.001 downsamples 17 most-common words\n2019-12-25 12:51:32,181 : INFO : downsampling leaves estimated 5 word corpus (14.5% of prior 36)\n2019-12-25 12:51:32,182 : INFO : estimated required memory for 17 words and 100 dimensions: 22100 bytes\n2019-12-25 12:51:32,182 : INFO : resetting layer weights\n2019-12-25 12:51:32,187 : INFO : training model with 3 workers on 17 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n2019-12-25 12:51:32,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 12:51:32,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 12:51:32,201 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 12:51:32,202 : INFO : EPOCH - 1 : training on 36 raw words (5 effective words) took 0.0s, 1294 effective words/s\n2019-12-25 12:51:32,211 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 12:51:32,212 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 12:51:32,212 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 12:51:32,213 : INFO : EPOCH - 2 : training on 36 raw words (5 effective words) took 0.0s, 2192 effective words/s\n2019-12-25 12:51:32,216 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 12:51:32,217 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 12:51:32,218 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 12:51:32,218 : INFO : EPOCH - 3 : training on 36 raw words (4 effective words) took 0.0s, 1674 effective words/s\n2019-12-25 12:51:32,243 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 12:51:32,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 12:51:32,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 12:51:32,249 : INFO : EPOCH - 4 : training on 36 raw words (8 effective words) took 0.0s, 1276 effective words/s\n2019-12-25 12:51:32,278 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 12:51:32,278 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 12:51:32,279 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 12:51:32,279 : INFO : EPOCH - 5 : training on 36 raw words (5 effective words) took 0.0s, 2322 effective words/s\n2019-12-25 12:51:32,280 : INFO : training on a 180 raw words (27 effective words) took 0.1s, 292 effective words/s\n2019-12-25 12:51:32,280 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\nC:\\Program Files\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:24: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n2019-12-25 12:51:32,331 : INFO : precomputing L2-norms of word weight vectors\nthis is the summary of the model: Word2Vec(vocab=17, size=100, alpha=0.025)\nThis is the vocab for our corpus: ['Mellissa', 'Alcantara', 'is', 'a', 'doctor', 'Hai', 'Nguyen', 'an', 'engineer', 'great', 'He', 'She', 'has', 'been', 'for', 'many', 'years']\nVector for the word doctor:\n[-3.95873608e-03  4.29156190e-03  1.19550910e-03  1.35508564e-03\n  2.60117743e-03  2.84649688e-03  9.71117930e-04  2.95825955e-03\n -2.19082832e-03  3.21021886e-03 -5.62904097e-05  6.02384447e-04\n -3.38134263e-03  2.44285911e-03 -1.02427904e-04  5.19171474e-04\n  2.91356957e-03 -3.99805652e-03 -4.34804475e-03 -2.92301620e-03\n  3.64908786e-03 -6.58127188e-04 -3.39436810e-03 -1.30981277e-03\n  2.52914289e-03 -1.29691849e-03  5.23347408e-04 -2.38239858e-03\n  8.57162522e-04 -2.98978994e-03  4.38950676e-03  1.73449211e-04\n -4.98803565e-03  2.74261949e-03 -4.90521127e-03  3.16288439e-03\n -3.20267456e-04 -3.54611129e-03 -6.45404682e-04 -1.86340476e-03\n  2.22957530e-03 -4.57736989e-03 -4.41777380e-03  3.15238116e-03\n  5.55609819e-04 -2.53596739e-03  8.88263530e-05  2.32096761e-03\n  1.32166955e-03  3.94581491e-03 -2.85362243e-03  2.32140324e-03\n -4.72674333e-03 -3.60556319e-03  2.86125299e-03  4.97092353e-03\n  4.60625812e-03 -4.86941123e-03  3.66341462e-03 -4.46950644e-03\n  8.09695630e-04 -3.56618525e-03  8.32282647e-04  1.31612120e-03\n  3.41841602e-03 -2.24839809e-04  4.30233916e-03 -2.21250555e-03\n  2.98772007e-03  9.45458130e-04  1.10729970e-03 -1.58305769e-03\n -4.15599532e-03  4.51440411e-03  2.35468359e-03  1.08812586e-03\n -1.91669026e-03 -3.95920128e-03 -1.52166595e-03  4.31762822e-03\n -3.88520467e-03  3.70136986e-04  3.92228179e-03 -4.91549820e-03\n -1.31406821e-03  3.59270768e-03  4.65908553e-03 -1.88200769e-03\n -5.45196817e-05 -4.83946083e-03 -1.11587264e-03  1.56553509e-03\n -2.02628132e-03  4.67839232e-03  1.24681462e-03  3.83302756e-03\n  3.94624146e-03  2.22711195e-03  2.89608422e-03  3.78837809e-03]\n"
    },
    {
     "data": {
      "text/plain": "[('He', 0.19533118605613708),\n ('engineer', 0.17807017266750336),\n ('Mellissa', 0.1594674289226532),\n ('Hai', 0.13192053139209747),\n ('an', 0.11084733158349991)]"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word embedding are: numerical representaions in the form of real-value vectors for text. Word embeddings can only be generated once Tokenization done. \n",
    "#Vectorization can by done by word embedding techniques. There are alot, but we ll only try Word2Vec[CBOW & SkipGram] and Glove\n",
    "# Exercise 8 : Word2Vec\n",
    "from gensim.models import Word2Vec as wtv\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Store 3 string with common words into 3 vars, tokenize each one and store all tokens in an array\n",
    "s1 = \"Mellissa Alcantara is a doctor\"\n",
    "s2 = \"Hai Nguyen is an engineer\"\n",
    "s3 = \"Mellissa is a great doctor\"\n",
    "s4 = \"He is a great engineer\"\n",
    "s5 = \"She has been a doctor for many years\"\n",
    "s6 = \"He has been an engineer for many years\"\n",
    "sentences = [word_tokenize(s1), word_tokenize(s2), word_tokenize(s3), word_tokenize(s4), word_tokenize(s5), word_tokenize(s6)]\n",
    "# sentences is 2D array\n",
    "# train model as follow, if no settings min_count is = 5\n",
    "model = wtv(sentences,min_count=1)\n",
    "# Summarize the model\n",
    "print('this is the summary of the model:', model)\n",
    "# To se the words in our vocab\n",
    "words = list(model.wv.vocab)\n",
    "print('This is the vocab for our corpus:', words)\n",
    "print(\"Vector for the word doctor:\")\n",
    "print(model['doctor'])\n",
    "# lookup top 6 similar words to great\n",
    "w1 = ['She']\n",
    "model.wv.most_similar(positive=w1, topn=5)\n",
    "############ Should Try more examples with Word2Vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'glove.Corpus'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-17646e90067c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#from glove import Corpus, Glove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mglove\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#3 Convert corpus into sentences in the form of a list by itertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'glove.Corpus'"
     ]
    }
   ],
   "source": [
    "# Exercise 9. Generating Word Embedding Using Glove \n",
    "# Install GloVE with: pip3 install https://github.com/JonathanRaiman/glove/archive/master.zip\n",
    "\n",
    "import itertools\n",
    "#2 We need a corpus to generate word embedding for, import Text8Corpus with 2 modules from Glove-Python\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "#from glove import Corpus, Glove\n",
    "import glove.Corpus\n",
    "import glove.Glove\n",
    "#3 Convert corpus into sentences in the form of a list by itertools\n",
    "sentencse = list(itertools.islice(Text8Corpus('text8'),None))\n",
    "\n",
    "#3 initiate the Corpus() model and fit it onto the sentences\n",
    "corpus = Corpus()\n",
    "corpus.fit(sentences, window = 10)\n",
    "\n",
    "#4 after prepared our corpus, need to train the the embeddings, initiate Glove()\n",
    "glove = Glove(no_components = 100, learning_rate = 0.05)\n",
    "\n",
    "#6 Generate co-occurence matrix based on the ocrpus and fit the glove Model on to this matrix\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads = 4, verbose = True)\n",
    "\n",
    "#7 add the dictionary of the corpus\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "#8 find neighbor\n",
    "glove.most_similar('man')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 11:21:00,453 : INFO : collecting all words and their counts\n2019-12-25 11:21:00,492 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n2019-12-25 11:21:05,121 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n2019-12-25 11:21:05,122 : INFO : Loading a fresh vocabulary\n2019-12-25 11:21:05,290 : INFO : effective_min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n2019-12-25 11:21:05,291 : INFO : effective_min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n2019-12-25 11:21:05,504 : INFO : deleting the raw counts dictionary of 253854 items\n2019-12-25 11:21:05,513 : INFO : sample=0.001 downsamples 38 most-common words\n2019-12-25 11:21:05,513 : INFO : downsampling leaves estimated 12506280 word corpus (74.8% of prior 16718844)\n2019-12-25 11:21:05,718 : INFO : estimated required memory for 71290 words and 200 dimensions: 149709000 bytes\n2019-12-25 11:21:05,718 : INFO : resetting layer weights\n2019-12-25 11:21:18,965 : INFO : training model with 3 workers on 71290 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n2019-12-25 11:21:19,978 : INFO : EPOCH 1 - PROGRESS: at 7.23% examples, 889055 words/s, in_qsize 5, out_qsize 1\n2019-12-25 11:21:20,984 : INFO : EPOCH 1 - PROGRESS: at 15.29% examples, 942711 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:21,985 : INFO : EPOCH 1 - PROGRESS: at 23.05% examples, 952026 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:22,993 : INFO : EPOCH 1 - PROGRESS: at 30.86% examples, 959550 words/s, in_qsize 6, out_qsize 0\n2019-12-25 11:21:24,000 : INFO : EPOCH 1 - PROGRESS: at 39.04% examples, 972058 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:25,001 : INFO : EPOCH 1 - PROGRESS: at 47.33% examples, 983547 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:26,005 : INFO : EPOCH 1 - PROGRESS: at 55.61% examples, 991402 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:27,005 : INFO : EPOCH 1 - PROGRESS: at 63.84% examples, 996342 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:28,005 : INFO : EPOCH 1 - PROGRESS: at 71.37% examples, 990511 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:29,007 : INFO : EPOCH 1 - PROGRESS: at 79.01% examples, 985312 words/s, in_qsize 4, out_qsize 0\n2019-12-25 11:21:30,008 : INFO : EPOCH 1 - PROGRESS: at 86.54% examples, 980956 words/s, in_qsize 6, out_qsize 0\n2019-12-25 11:21:31,009 : INFO : EPOCH 1 - PROGRESS: at 94.18% examples, 978531 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:31,729 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 11:21:31,739 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 11:21:31,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 11:21:31,744 : INFO : EPOCH - 1 : training on 17005207 raw words (12504421 effective words) took 12.8s, 978826 effective words/s\n2019-12-25 11:21:32,756 : INFO : EPOCH 2 - PROGRESS: at 8.11% examples, 998837 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:33,762 : INFO : EPOCH 2 - PROGRESS: at 16.05% examples, 990447 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:34,768 : INFO : EPOCH 2 - PROGRESS: at 24.22% examples, 1000142 words/s, in_qsize 4, out_qsize 0\n2019-12-25 11:21:35,772 : INFO : EPOCH 2 - PROGRESS: at 32.16% examples, 999776 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:36,774 : INFO : EPOCH 2 - PROGRESS: at 39.80% examples, 992281 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:37,778 : INFO : EPOCH 2 - PROGRESS: at 47.56% examples, 988628 words/s, in_qsize 4, out_qsize 0\n2019-12-25 11:21:38,780 : INFO : EPOCH 2 - PROGRESS: at 55.14% examples, 983377 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:39,787 : INFO : EPOCH 2 - PROGRESS: at 63.20% examples, 985921 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:40,795 : INFO : EPOCH 2 - PROGRESS: at 71.43% examples, 990238 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:41,795 : INFO : EPOCH 2 - PROGRESS: at 79.66% examples, 992481 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:42,798 : INFO : EPOCH 2 - PROGRESS: at 87.95% examples, 995996 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:43,809 : INFO : EPOCH 2 - PROGRESS: at 95.59% examples, 991628 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:44,377 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 11:21:44,377 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 11:21:44,383 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 11:21:44,384 : INFO : EPOCH - 2 : training on 17005207 raw words (12506581 effective words) took 12.6s, 989673 effective words/s\n2019-12-25 11:21:45,394 : INFO : EPOCH 3 - PROGRESS: at 7.35% examples, 907295 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:46,394 : INFO : EPOCH 3 - PROGRESS: at 15.46% examples, 958649 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:47,397 : INFO : EPOCH 3 - PROGRESS: at 23.10% examples, 957563 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:48,407 : INFO : EPOCH 3 - PROGRESS: at 30.81% examples, 959654 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:49,408 : INFO : EPOCH 3 - PROGRESS: at 39.09% examples, 976358 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:50,415 : INFO : EPOCH 3 - PROGRESS: at 47.44% examples, 987117 words/s, in_qsize 4, out_qsize 0\n2019-12-25 11:21:51,422 : INFO : EPOCH 3 - PROGRESS: at 55.67% examples, 992907 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:52,424 : INFO : EPOCH 3 - PROGRESS: at 63.96% examples, 998565 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:53,430 : INFO : EPOCH 3 - PROGRESS: at 72.37% examples, 1003959 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:54,432 : INFO : EPOCH 3 - PROGRESS: at 80.83% examples, 1007541 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:55,438 : INFO : EPOCH 3 - PROGRESS: at 89.07% examples, 1008990 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:56,446 : INFO : EPOCH 3 - PROGRESS: at 97.53% examples, 1011826 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:56,738 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 11:21:56,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 11:21:56,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 11:21:56,751 : INFO : EPOCH - 3 : training on 17005207 raw words (12507031 effective words) took 12.4s, 1011666 effective words/s\n2019-12-25 11:21:57,756 : INFO : EPOCH 4 - PROGRESS: at 8.35% examples, 1034373 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:58,769 : INFO : EPOCH 4 - PROGRESS: at 16.81% examples, 1037333 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:21:59,773 : INFO : EPOCH 4 - PROGRESS: at 25.16% examples, 1039302 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:00,775 : INFO : EPOCH 4 - PROGRESS: at 33.27% examples, 1036184 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:01,776 : INFO : EPOCH 4 - PROGRESS: at 41.62% examples, 1038465 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:02,784 : INFO : EPOCH 4 - PROGRESS: at 49.91% examples, 1037604 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:03,784 : INFO : EPOCH 4 - PROGRESS: at 58.08% examples, 1035909 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:04,789 : INFO : EPOCH 4 - PROGRESS: at 65.49% examples, 1021921 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:05,797 : INFO : EPOCH 4 - PROGRESS: at 73.37% examples, 1017637 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:06,799 : INFO : EPOCH 4 - PROGRESS: at 81.36% examples, 1013665 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:07,801 : INFO : EPOCH 4 - PROGRESS: at 89.07% examples, 1008943 words/s, in_qsize 4, out_qsize 0\n2019-12-25 11:22:08,805 : INFO : EPOCH 4 - PROGRESS: at 96.71% examples, 1003832 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:09,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 11:22:09,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 11:22:09,212 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 11:22:09,212 : INFO : EPOCH - 4 : training on 17005207 raw words (12504815 effective words) took 12.5s, 1003697 effective words/s\n2019-12-25 11:22:10,225 : INFO : EPOCH 5 - PROGRESS: at 7.70% examples, 946343 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:11,232 : INFO : EPOCH 5 - PROGRESS: at 15.29% examples, 942300 words/s, in_qsize 4, out_qsize 1\n2019-12-25 11:22:12,237 : INFO : EPOCH 5 - PROGRESS: at 23.28% examples, 960812 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:13,249 : INFO : EPOCH 5 - PROGRESS: at 30.92% examples, 959350 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:14,255 : INFO : EPOCH 5 - PROGRESS: at 38.33% examples, 953080 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:15,256 : INFO : EPOCH 5 - PROGRESS: at 46.38% examples, 962709 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:16,264 : INFO : EPOCH 5 - PROGRESS: at 54.56% examples, 970903 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:17,270 : INFO : EPOCH 5 - PROGRESS: at 62.43% examples, 972080 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:18,275 : INFO : EPOCH 5 - PROGRESS: at 70.49% examples, 975890 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:19,277 : INFO : EPOCH 5 - PROGRESS: at 78.48% examples, 976598 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:20,281 : INFO : EPOCH 5 - PROGRESS: at 86.60% examples, 979486 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:21,287 : INFO : EPOCH 5 - PROGRESS: at 94.77% examples, 982240 words/s, in_qsize 5, out_qsize 0\n2019-12-25 11:22:21,970 : INFO : worker thread finished; awaiting finish of 2 more threads\n2019-12-25 11:22:21,978 : INFO : worker thread finished; awaiting finish of 1 more threads\n2019-12-25 11:22:21,980 : INFO : worker thread finished; awaiting finish of 0 more threads\n2019-12-25 11:22:21,981 : INFO : EPOCH - 5 : training on 17005207 raw words (12506644 effective words) took 12.8s, 979787 effective words/s\n2019-12-25 11:22:21,982 : INFO : training on a 85026035 raw words (62529492 effective words) took 63.0s, 992303 effective words/s\nC:\\Program Files\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n  if __name__ == '__main__':\n2019-12-25 11:22:21,983 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "data": {
      "text/plain": "[('queen', 0.6347061395645142)]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More Word2Vec in Python\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    " \n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    " \n",
    "sentences = word2vec.Text8Corpus('text8')\n",
    " \n",
    "model = word2vec.Word2Vec(sentences, size=200)\n",
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Program Files\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n  \"\"\"Entry point for launching an IPython kernel.\n"
    },
    {
     "data": {
      "text/plain": "[('queen', 0.6347061395645142), ('throne', 0.5590610504150391)]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['woman', 'king'], negative=['man'], topn=2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "C:\\Program Files\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n  \"\"\"Entry point for launching an IPython kernel.\n"
    },
    {
     "data": {
      "text/plain": "[('woman', 0.7014908194541931),\n ('girl', 0.5944930911064148),\n ('creature', 0.5530924201011658),\n ('person', 0.5235102772712708),\n ('gentleman', 0.5167765617370605),\n ('boy', 0.507409393787384),\n ('stranger', 0.5057109594345093),\n ('bride', 0.5049154758453369),\n ('god', 0.4987950623035431),\n ('men', 0.4826430678367615)]"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(['man'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 11:25:14,157 : INFO : saving Word2Vec object under text8.model, separately None\n2019-12-25 11:25:14,158 : INFO : storing np array 'vectors' to text8.model.wv.vectors.npy\n2019-12-25 11:25:14,908 : INFO : not storing attribute vectors_norm\n2019-12-25 11:25:14,911 : INFO : storing np array 'syn1neg' to text8.model.trainables.syn1neg.npy\n2019-12-25 11:25:15,455 : INFO : not storing attribute cum_table\n2019-12-25 11:25:15,602 : INFO : saved text8.model\n"
    }
   ],
   "source": [
    "model.save('text8.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 11:26:35,948 : INFO : storing 71290x200 projection weights into text.model.bin\n"
    }
   ],
   "source": [
    "# save model to .bin format\n",
    "model.wv.save_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 11:28:19,792 : INFO : loading projection weights from text.model.bin\n2019-12-25 11:28:20,986 : INFO : loaded (71290, 200) matrix from text.model.bin\n"
    }
   ],
   "source": [
    "# loading model .bin format \n",
    "model1 = gensim.models.KeyedVectors.load_word2vec_format('text.model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 11:28:35,089 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "data": {
      "text/plain": "[('mother', 0.7539545297622681),\n ('wife', 0.720678448677063),\n ('grandmother', 0.69264155626297)]"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The activity one, 'Father' is to 'girl' , 'x' is to 'boy'. \n",
    "model1.most_similar(['girl', 'father'], ['boy'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'he' is to 'is' as 'she' is to 'exists'\n'big' is to 'bigger' as 'bad' is to 'worse'\n'going' is to 'went' as 'being' is to 'was'\nC:\\Program Files\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n  after removing the cwd from sys.path.\n"
    }
   ],
   "source": [
    "more_examples = [\"he is she\", \"big bigger bad\", \"going went being\"] \n",
    "for example in more_examples:\n",
    "    a, b, x = example.split()\n",
    "    predicted = model.most_similar([x, b], [a])[0][0]\n",
    "    print (\"'%s' is to '%s' as '%s' is to '%s'\" % (a, b, x, predicted))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "2019-12-25 11:57:12,853 : INFO : loading projection weights from .\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n2019-12-25 11:58:16,073 : INFO : loaded (3000000, 300) matrix from .\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin\n2019-12-25 11:58:16,074 : INFO : precomputing L2-norms of word weight vectors\n"
    },
    {
     "data": {
      "text/plain": "[('frogs', 0.8018162250518799),\n ('toad', 0.7049819231033325),\n ('lizard', 0.6754182577133179),\n ('Aricent_combines', 0.6699402928352356),\n ('bullfrog', 0.6557111144065857),\n ('critter', 0.6445190906524658),\n ('amphibian', 0.641465961933136),\n ('salamander', 0.6264126300811768),\n ('turtle', 0.6156665086746216),\n ('snail', 0.6094340085983276)]"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_org = gensim.models.KeyedVectors.load_word2vec_format('.\\GoogleNews-vectors-negative300.bin\\GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model_org.most_similar('frog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('Queen', 0.5645124912261963),\n ('Geoffrey_Rush_Exit', 0.4681748151779175),\n ('Greene', 0.4668373763561249)]"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_org.most_similar(['girl', 'King'], ['boy'], topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  https://textminingonline.com/getting-started-with-word2vec-and-glove-in-python\n",
    "\n",
    "# https://www.programcreek.com/python/example/98849/gensim.models.word2vec.Text8Corpus"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}