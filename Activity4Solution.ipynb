{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n                                         comment_text toxic\n0   \"\\r\\n \\r\\n I'm sure if you worked on articles ...     0\n1   D'aww! He matches this background colour I'm s...     0\n2   Hey man, I'm really not trying to edit war. It...     0\n3   \"\\r\\n More\\r\\n I can't make any real suggestio...     0\n4    REDIRECT Talk:Voydan Pop Georgiev- Chernodrinski     0\n..                                                ...   ...\n95  Because I find it funny - I was blocked five m...     0\n96  \"\\r\\n \\r\\n  Hi \\r\\n \\r\\n He has made veiled th...     0\n97  (To ), Actually, they did noy mess anything up...     0\n98  \"Citation 2 has reported false information on ...     0\n99                                       \"No comment\"     0\n\n[100 rows x 2 columns]\nX size is  100\ny size is  100\nUsing TensorFlow backend.\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 20)                420       \n_________________________________________________________________\ndense_2 (Dense)              (None, 20)                420       \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 21        \n=================================================================\nTotal params: 861\nTrainable params: 861\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/5\n80/80 [==============================] - 0s 1ms/step - loss: -730.4372 - accuracy: 0.0125 \nEpoch 2/5\n80/80 [==============================] - 0s 337us/step - loss: -730.4372 - accuracy: 0.0125\nEpoch 3/5\n80/80 [==============================] - 0s 274us/step - loss: -730.4372 - accuracy: 0.0125\nEpoch 4/5\n80/80 [==============================] - 0s 311us/step - loss: -730.4371 - accuracy: 0.0125\nEpoch 5/5\n80/80 [==============================] - 0s 312us/step - loss: -730.4372 - accuracy: 0.0125\n20/20 [==============================] - 0s 798us/step\nAccuracy: 1.0\nscores: [0.0, 1.0]\n"
    }
   ],
   "source": [
    "#1. Import numpy, panda and matplotlib.py, load dataset to dataframe \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "dataset = pd.read_csv('./Exercise17/train_comment_small_100.csv',sep=',')\n",
    "#2. prepare to clean data. \n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# Create array for cleaned text to be stored. \n",
    "corpus = []\n",
    "values = {'comment_text': \"No comment\", 'toxic': 1}\n",
    "dataset['comment_text'] = dataset.comment_text.fillna('\"No comment\"')\n",
    "dataset['toxic'] = dataset.toxic.fillna('0')\n",
    "print(dataset)\n",
    "#3. Using loop, iterate through every instance. Replace all non-alphanumeric with a ''(whitespace). Convert all to lowercases. Split review into individual words, initiate PorterSetemmer.  \n",
    "# Looking at the keyword as two function:\n",
    "# [(Pattern)] = match(Pattern)\n",
    "# [^(Pattern)] = notMatch(Pattern)\n",
    "# Moreover regarding a pattern:\n",
    "# A-Z = all characters included from A to Z\n",
    "# a-z = all characters included from a to z\n",
    "# 0-9 = all characters included from 0 to 9\n",
    "\n",
    "regex = re.compile('[^a-zA-Z\\s]+')\n",
    "for i in range(0, dataset.shape[0]):\n",
    "    # \"[^a-zA-Z]+\" - look for any group of characters that are NOT a-zA-z.\n",
    "    # review = re.sub('[^a-zA-Z]'),' ',dataset['comment_text'][i])\n",
    "    review = regex.sub(' ',dataset['comment_text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer() \n",
    "    #if the word is not stopword=> Stemming, join the clean words to get a clean comment\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "\n",
    "#4. Use Vectorizer, convert review to word counts vectors.why max is 20 /1500\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer( max_features = 20)   # 1500 related to input_dim?\n",
    "#5. Create an array to store each unique word as its own column, make them independent variables\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "\n",
    "# # Columns: https://www.shanelynn.ie/select-pandas-dataframe-rows-and-columns-using-iloc-loc-and-ix/\n",
    "# data.iloc[:,0] # first column of data frame (first_name)\n",
    "y = dataset.iloc[:,0]\n",
    "y1 = y[:100]\n",
    "\n",
    "print(\"X size is \",len(corpus))\n",
    "print(\"y size is \",y1.size)\n",
    "#6. Import Label Encoder, from sklearn.preprocessing, use it for target output (y)\n",
    "from sklearn import preprocessing\n",
    "LabelEncoder_y = preprocessing.LabelEncoder()\n",
    "y = LabelEncoder_y.fit_transform(y1)\n",
    "\n",
    "#7. Import train_test_split, prepare data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state = 0)\n",
    " \n",
    "#8. Import StandardScaler from sklearn.preprocessing ?\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "Sc = StandardScaler()\n",
    "X_train = Sc.fit_transform(X_train)\n",
    "X_test = Sc.fit_transform(X_test)\n",
    "\n",
    "#10 Import the model and layers from Keras\n",
    "import tensorflow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#Initialize Neural Network \n",
    "Neural_Net = Sequential()\n",
    "# Add the hidden layer. output_dim ?\n",
    "Neural_Net.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu', input_dim = 20))\n",
    "Neural_Net.add(Dense(output_dim = 20, init = 'uniform', activation = 'relu'))\n",
    "# Add the output layer. \n",
    "Neural_Net.add(Dense(output_dim = 1, init = 'uniform', activation = 'softmax'))\n",
    "#Compile the NN, decide which loss function, optimazation algorithm and performance metric we want to use. \n",
    "Neural_Net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#Summarize our model\n",
    "Neural_Net.summary()\n",
    "\n",
    "#11 Train the model, ? nb_epoch 5 ? batch_size 3\n",
    "Neural_Net.fit(X_train,y_train,nb_epoch=5, batch_size= 3)\n",
    "X_test\n",
    "\n",
    "#12 Evaluate the model \n",
    "y_predict= Neural_Net.predict(X_test)\n",
    "scores = Neural_Net.evaluate(X_test, y_predict, verbose=1)\n",
    "print(\"Accuracy:\",scores[1])\n",
    "print(\"scores:\",scores)\n",
    "\n",
    "# #13.(optional) Print the confusion matrix by importing confusion_matrix\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# cm = confusion_matrix(y_test,y_predict)\n",
    "# scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/35977551/regex-for-keeping-only-letters-and-blank-space.\n",
    "# [^A-Za-z\\s]+ for:\n",
    "# any character, except [captial letters; lower case letters; whitespaces (blanks, tabs, linebreaks)]\n",
    "stringA = \"A145nh H7'@a'''i124 is   wonderful guy\"\n",
    "regex = re.compile('[^a-zA-Z\\s]+')\n",
    "\n",
    "stringA = regex.sub('',stringA)\n",
    "print(stringA)\n",
    "stringA = stringA.split()\n",
    "stringA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test,y_predict)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "99"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape[0]-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('virtual_env': venv)",
   "language": "python",
   "name": "python37464bitvirtualenvvenv9acec58b124a4af58980f597288afade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}