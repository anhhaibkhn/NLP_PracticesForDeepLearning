{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 30, 50)            84850     \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 26, 64)            16064     \n_________________________________________________________________\nglobal_max_pooling1d_2 (Glob (None, 64)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 10)                650       \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 11        \n=================================================================\nTotal params: 101,575\nTrainable params: 101,575\nNon-trainable params: 0\n_________________________________________________________________\nTrain Loss: 0.0009\nTrain Accuracy:1.0000\nTest Loss: 0.6991\nTest Accuracy: 0.7767\n"
    }
   ],
   "source": [
    "#Activity 5. Sentiment Analysis on a RealLife Data-Set\n",
    "#1. Import DATA using pandas read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "here = os.path.dirname(__file__) if \"__file__\" in locals() else \".\"\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
    "\n",
    "# files = [(\"amazon\", os.path.join(here, \"./Sentiment_labelled_sentences//amazon_cells_labelled.txt\")),\n",
    "#          (\"imdb\", os.path.join(here, \"./Sentiment_labelled_sentences//imdb_labelled.txt\")),\n",
    "#          (\"yelp\", os.path.join(here, \"./Sentiment_labelled_sentences//yelp_labelled.txt\"))]\n",
    "\n",
    "files = [(\"amazon\", os.path.join(here, \"./Sentiment_labelled_sentences//yelp_labelled.txt\"))]\n",
    "\n",
    "# df = pd.read_csv('./Sentiment_labelled_sentences/amazon_cells_labelled.txt', delimiter=\"\\t\",sep=\" \", header=None)\n",
    "# df.columns = [\"Sentence\", \"Label\"]\n",
    "dfs = [] # define an empty array\n",
    "for provider, name in files:\n",
    "    df = pd.read_csv(name, sep=\"\\t\")\n",
    "    df.columns = [\"Sentence\", \"Label\"]\n",
    "    df[\"provider\"] = provider\n",
    "    dfs.append(df)\n",
    "#dfs now has 3 elements which are 3 arrays in 3 columns\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html\n",
    "Data = pd.concat(dfs,axis = 0) # join 3 framses \n",
    "# print(Data.head(10))\n",
    "# print(\"shape\", Data.shape)\n",
    "import re\n",
    "#1.2 Create a func to preproces the reviews (clean and prepare data)\n",
    "def clean_comment(text):\n",
    "    #Strip HTML tags\n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    "    #Strip escaped quotes\n",
    "    text = text.replace('\\\\\"', '')\n",
    "    # text = re.sub('//\"', '', text)\n",
    "    #Strip quotes\n",
    "    text = text.replace('\"', '')\n",
    "    # text = re.sub('\"', '', text)\n",
    "    return text\n",
    "Data = Data.dropna()\n",
    "Data[\"cleaned_cmts\"] = Data[\"Sentence\"].apply(clean_comment) \n",
    "\n",
    "####? Should we add Lower casing and Removing stop_words\n",
    "# print(Data.Label.value_counts()) , count positive and negative comments\n",
    "\n",
    "#1.3 Define variables \n",
    "Batch_size = 32\n",
    "Epochs = 20\n",
    "Maxlen = 30  # Pre-define Vocabulary size, this size affects heavily on the training time\n",
    "Embedded_Dim = 50 # Embedding Dimension, output vector\n",
    "Num_filters = 64\n",
    "Kernel_Size = 5 \n",
    "\n",
    "#2. Split Data to Train and Test using scikit-learn'S train_test_split \n",
    "import numpy as np\n",
    "from sklearn.model_selection import  train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Data[\"cleaned_cmts\"],Data[\"Label\"], test_size=0.30, random_state = 1000) # random_state: keep the data the same evertime it runs\n",
    "\n",
    "# print(max_len)\n",
    "# for x in range(len(X_train)): \n",
    "#     print(\"element \",x,\"has len of \",len(X_train[x])) \n",
    "\n",
    "#3. Tokenize using Keras's tokenizer \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=100000) # num_words: the maximum number of words to keep, Bigger -> Better\n",
    "# Build Vocab based on training data ? < What if Test data has the words that not in Training Data \n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "\n",
    "#4.Use texts_to_sequences: Transforms each text in texts to a sequence of integers  \n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "Vocab_size =  len(tokenizer.word_index) + 1 # The Vocab size has an additional 1, due to 0 index\n",
    "\n",
    "#5.  Ensure that all the sequencese have the same length by padding them, using Keras's pad_sequences function\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "X_train_padded = pad_sequences(X_train, padding = 'post', truncating = 'post', maxlen = Maxlen) \n",
    "# \n",
    "X_test_padded = pad_sequences(X_test, padding = 'post', truncating = 'post', maxlen = Maxlen)\n",
    " # , maxlen=20)\n",
    "# for count in range(6):\n",
    "#  print(X_test_padded[count])\n",
    "\n",
    "\n",
    "#6.  Define Model with minimum 1 ConV, 1 Fully Connected, using 'sigmoid' as activation function, calculate the loss to cross-entropy loss\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import plot_model\n",
    "\n",
    "MODEL = Sequential()\n",
    "MODEL.add(layers.Embedding(input_dim = Vocab_size, output_dim= Embedded_Dim, input_length = Maxlen)) # traineble = false, no update to learned data. \n",
    "MODEL.add(layers.Conv1D(Num_filters, Kernel_Size, activation='relu'))\n",
    "MODEL.add(layers.GlobalMaxPooling1D())\n",
    "MODEL.add(layers.Dense(units = 10, activation='relu'))\n",
    "MODEL.add(layers.Dense(units = 1, activation = 'sigmoid'))\n",
    "MODEL.compile(loss ='binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "MODEL.summary()\n",
    "\n",
    "History = MODEL.fit(X_train_padded,y_train, epochs = Epochs, batch_size = Batch_size, verbose = 0, validation_data =(X_test_padded,y_test))\n",
    "\n",
    "Score = MODEL.evaluate(X_train_padded,y_train, batch_size = Batch_size, verbose = 0)\n",
    "print(\"Train Loss: {:.4f}\".format(Score[0]))\n",
    "print(\"Train Accuracy:{:.4f}\".format(Score[1]))\n",
    "\n",
    "Score1 = MODEL.evaluate(X_test_padded,y_test, batch_size = Batch_size, verbose = 0)\n",
    "print(\"Test Loss: {:.4f}\".format(Score1[0]))\n",
    "print(\"Test Accuracy: {:.4f}\".format(Score1[1]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "word_index :  {'this': 1, 'girl': 2, 'is': 3, 'looking': 4, 'beautiful': 5}\n[[1, 2, 3, 4, 5]]\n[[1, 2, 3, 4, 5]]\n[[1, 2, 3, 5]]\n[[2], [1, 2, 3, 4, 5]]\n"
    }
   ],
   "source": [
    "# Bonus 1: example to understand Tokenizer and Text_to_sequences method of Keras \n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tok = Tokenizer()\n",
    "train_text = [\"this girl is looking beautiful!!\"] \n",
    "test_text = [\"this girl is not really looking beautiful\"]\n",
    "test_text2 = [\"this girl is beautiful\"]\n",
    "test_text3 = \"bad girl\"\n",
    "test_text4 = \"This girl is not really looking beautiful and nice\"\n",
    "# This is for updating internal vocabulary based on a list of texts\n",
    "tok.fit_on_texts(train_text) \n",
    "print(\"word_index : \",tok.word_index)\n",
    "\n",
    "#texts_to_sequences Transforms each text in texts to a sequence of integers.\n",
    "tr1 = tok.texts_to_sequences(train_text) \n",
    "sequences1 = tok.texts_to_sequences(test_text)\n",
    "sequences2 = tok.texts_to_sequences(test_text2)\n",
    "\n",
    "sequences34 = tok.texts_to_sequences([test_text3 ,test_text4])\n",
    "# t4 = tok.texts_to_sequences(test_text4)\n",
    "print(tr1) # Vocabulary was built with 5 words \n",
    "print(sequences1)  # it only recognized the words in Vocab\n",
    "print(sequences2)  # all the word are already in the Vocab \n",
    "print(sequences34) # \"Bad\" was not recognized \n",
    "# print(t4) # It can not recognized the words which are not in the Vocab \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "  0   0   0   0   0   0]\n[  13    6    1 1200   11    2  354    7    5 1655    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 80  20 142 905  66   1   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[26 28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[  5 218   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 13   5  30 117   5  15   6 173  33   6 256 333  14   5 213   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  4  43 108 214  32  77  26  12  56 424   2 285  82  85  26 178   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4 1531    1  525   30   87   13  833  148    4  332    1  121   87\n    5  178  359    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[1662   11   22   76   11    1  104  217  885    1    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 54  34 382   2  13 147   1 291   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 10   7  69 459 165  58  22  76  11   1  48   9   4  31  49 237   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[182   8   5 197  32  47  24 172  32  87   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  2   8   3 186   6   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  6  23 361 185  50   1 104  11 513   4  31  49 237   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  4 214  78 129  30   4 238   6  24 144 682 144 874   2 461   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 67  81  47   4  24 255  35  10  15   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1   7  44 167   2  19  68   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  23   14  395   35    1  652   36 1421  166    2  650   57   93    3\n  675   40  137  127    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  11   40   93  289  219   14 1037   11  116 1553   11  462    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 13  23 187   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  14   69 1554 1429   11  236   10   15    7   64    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[216   3   5   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  32  222 1665  125 1308    1   59    9    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1 442   3 177   1 104  80 862  14  75  42   3 505   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  4  39 148  85   2   8  39   5 215   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 122  34 162  13  50  71   1 124 119  12 118 283  34 462  70  52  98\n  12   1 106 526 110   0   0   0   0   0   0   0]\n[324 117  24  14 349  81   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[442   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[107   6  56 833 761   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 112   4 105   3 117   2   3  61 676   1  29 211   2 589   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1  48  15  14  75  12 155  44 274  72   5 357  89   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  9 139 113  62 462   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[278  31  61 440   2   1   9   3 252  59 361  41   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   1 1275  279  226   22  203    7   69  315  226   50   91    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 19  71 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  40   14   40    4  222 1381   32  324   24   33    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[214  13 377  56 304  36   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4   53  361  115  314   30    4   78    1  582 1676   94    2  623\n 1513   36    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[267 395   6  24   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4  323  278   66    5  129   30    4  222  117  342    1 1148   16\n    9    2  102    9    2   10    3  102    9    0    0    0    0    0\n    0    0]\n[ 10  15   7 266 687  63 186  63   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 50  12   1  18  88   5 148  85 360   2 465 118 283  60  76  68 572   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  44  204  221  168 1538    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 13  98   5  17  47  24 219  22  32   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[50 12  1 55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[  4  87  13 380  10   6  24  26  16   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  8   3 423 127 211  13 221 140  89  44 102   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 19  16 206 188   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  51   13  223    6 1208    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  28   47   56  151  179  186 1581   39   23    7 1122   22  478  844\n   78    8    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  10    7   52   11    1   48 1423   22    9   14   75    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[1554  632    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1   9 126  16   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  6   1   9   3 590  30 239 116 260  39   1  11   1   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4  891  657  418   50  115   50 1375  139  999    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[242  25   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[10  7 69  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[  1  54  88  70  11 413  10  15   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[25  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[   4  238    5 1216  325  495   11   14   23    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1 122  34  55  19  71   2   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   5  174 1680    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1 154  36   7 378   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  67   20  921    6 1657  100   36    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[   1    7   72   11   10 1643    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[32 83 24 74  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[136   2 871   7   5  16 186  11   1 331 224   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1  90  20  26 115  16  27   8   4  79  13   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 57   5 861  21   3   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[415   7  19 191 315 243  18   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 34  43  25 217   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[18  7 55  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[  44   50   16   50   82    4   20    8   84   96    5 1217  903    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  4  51 223 531  12  10   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 19  19 817   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4   53   31   19  190    6  107  100   10   15   30    8 1203  120\n   73   91    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[475 153   5  21 103  72   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 80  20 142 154   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 82   4  23 631   8   3 404   8  87  31   5 235  11 224  14   8  26 687\n  63  42   0   0   0   0   0   0   0   0   0   0]\n[  4  31  62  36 902 119  14   1   2   1  86 113 133  62  25   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[256  41   4  95  36   4 264   6   5   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 18   3 174   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 1 18 36  7 35 48  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[ 13 115 140   6 125   2  19   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 17 146  12 110   6  24 318 814  42  29 470 321   2  17  29   1  61 572\n 400   0   0   0   0   0   0   0   0   0   0   0]\n[   4 1476   10   15  787  306    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  4 332  39   4  20  67 334 600   9  10 185   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[153  69   2 180 435 110   2 152   1 160 158 526 110   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[196 111  12   1 978 979 352   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 44  20 206  36   2  20   5  25  86   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4   20    6  124  144  307  110    6   77   23  581    2 1693    6\n   77  267    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[   5 1664   11    4    2   20   69   64  218    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[   1  141    2    1  434   34   14   23    1   48 1173   35    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 32  37 419  22 179  11   1   9  36   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[200   4   3  13 262   2  51  13  37  33   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 10  15 113   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[108 222  32 620   5  45  32 175   5 141   4  53   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[104 600  49   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   1  122    7   25    1    9    7 1248    2   28   31   69  590  376\n  442    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1 244   7 205   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 14  10   3   5 511 255  86   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 10  15 594  70 111   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[426  31  62  69 192 137  35  10  15   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 45  32 968  36  37 162   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 23 105   1   2  28 128 371 127   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 82   4   1 226   4   3 262  30  13  14   5  16 186   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 69 426 601   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 372  34  26 130   2   1 203  39 290 600   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 209  29  13 189   2   3  23 208   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 14 417  88 101   6  63   6 388 111  44 148 475   3  26 170   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[  46  112  373  141    2  888    7   23   40   41 1481    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[834  31   5 115 142  18  66   1 160  15 167  96   1  17  66  10  58   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 298 196   2 267 399   2 145 799   2  57  17 483   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 48  18   2   9  49  38 121   3  26  16   2  71 475  94  38 166   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  8   3 252  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 53 214   8   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 10   3  23 106  41   2   4 175 124   1 167   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 16 376 581 442   2  16   9 442   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  4 351  10  15   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 18   7 360   2  71   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 1 34 40  2 59  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[219   1 809  10   7  26  16   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 629  21 103  22  23 218  29  72   2   2   1   9   3 136   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[274   8  72   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  17  357  158 1546  110    2 1285  490   63    2  259    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1  18   3 248   2 118  68   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[51 13 37 33  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n  0  0  0  0  0  0]\n[   4  819   28   83 1080    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[   1 1051  579 1085   73    1    7   73   30    4 1286   95   36   81\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1 285   2  29  43  16   1   3  19 130   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1  34 252  64   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1   3   2 623   2 529   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   4   63    6   24 1005   26    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 117  147  192    1  706   12    1  433 1177   47  390  167   41   88\n   14   75    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 510  123   23 1481   58    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  4 105   9  36  35   1 157   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  1 208   4 105   1   5 208   2   3 611  11  74   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 10  15   7  73  16  68 165 612  14   1  58   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 69 253 261   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[242  11   1 539 506  29 174   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   1 1204   34    5  426   31    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[   1    7   44  221    6 1171    1    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  23  106  601    6    3    5 1140    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1 112   3  55  19 138   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 19 511   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[  82   23 1575    2    4  105  353  475  550  105  675    2  475   61\n   20    5  201   11  109    0    0    0    0    0    0    0    0    0\n    0    0]\n[  1   9   7  59   2  44 204 221  26  24 225   6 366  12  45  32   8  21\n 186   0   0   0   0   0   0   0   0   0   0   0]\n[ 40   4  31   6 107   7   1   9   3  64   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   1    9    3  117   13 1099   35   40    2   12    1  291   32  564\n   32  294  380   84    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 30   1  18   3 102   0   0   0   0   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[   1  354    7 1326   16    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[ 10   3  23 106  86   2   8   3  59   0   0   0   0   0   0   0   0   0\n   0   0   0   0   0   0   0   0   0   0   0   0]\n[ 220    6    2    1   18    3 1399    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n[1382    7  168    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0]\n"
    }
   ],
   "source": [
    "\n",
    "    print(*X_train_padded, sep = \"\\n\")\n",
    "    print(*X_test_padded, sep = \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('virtual_env': venv)",
   "language": "python",
   "name": "python37464bitvirtualenvvenv9acec58b124a4af58980f597288afade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}