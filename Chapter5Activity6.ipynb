{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Number of characters: 52\nauthor A sequences: (216441, 30)\nauthor B sequences: (230897, 30)\nTotal word count:  74412\nTotal number of unique words:  6337\n"
    }
   ],
   "source": [
    "# Chapter5. Author Attribution. Solution \n",
    "\n",
    "################################################# STEP 1/ Prepare the data.#################################\n",
    "import fileinput     \n",
    "import glob \n",
    "import os\n",
    "here = os.path.dirname(__file__) if \"__file__\" in locals() else \".\"\n",
    "files = [(\"AuthorA\", os.path.join(here, \"./papers//A//*.txt\")),\n",
    "         (\"AuthorB\", os.path.join(here, \"./papers//B//*.txt\"))]\n",
    "\n",
    "\"\"\"preprocess_text: Gets the contents of all .txt files from a folder and join them to a long string\n",
    "--------------- Paras: --------------------------------------------------------------------------\n",
    "@Author: The output file Name \n",
    "@DirName: Str indicates file location\n",
    "--------------- Returns:-------------------------------------------------------------------------\n",
    "@Text: a long tring representing the joined files contents \n",
    "\"\"\"\n",
    "def preprocess_text(Author,DirName):\n",
    "    read_files = glob.glob(DirName)\n",
    "    # .read() method of a file handle \n",
    "    # reads the contents of a file and produces a long string\n",
    "    with open(Author +\".txt\", \"wb\") as outfile:\n",
    "        for file in read_files:\n",
    "            with open(file, \"rb\") as infile:\n",
    "                outfile.write(infile.read().lower())\n",
    "        # Read the text with long string and lowercase it then export it \n",
    "        Tempfile = open(Author +\".txt\", 'rt').read().replace('hamilton','').replace('madison','')\n",
    "        Text = ' '.join(Tempfile.replace(\"\\n\",' ').split()) # rmv newlines, whitespaces\n",
    "        # write to file IF Need\n",
    "        # print(Text,  file=open(Author +'.txt', 'w'))\n",
    "        # print(Author + \" text length is: {}\".format(len(Text))) #print text length\n",
    "    return Text\n",
    "\n",
    "All_Text = {} # Dynamic variable in python \n",
    "for Author, DirName in files:\n",
    "    All_Text[Author] = preprocess_text(Author, DirName)\n",
    "    # print(Author + \" text length is: {}\".format(len(All_Text[Author]))) \n",
    "# # Access Dynamic variable in python \n",
    "# print(\"A text length is: {}\".format(len(All_Text[\"AuthorA\"]))) #print text length\n",
    "# print(\"B text length is: {}\".format(len(All_Text[\"AuthorB\"]))) #print text length \n",
    "\n",
    "###################### STEP 2/Break the long text for each author into smaller sequences.####################\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "SEQ_LEN = 30 \n",
    "# Classes for A/B/Unknown\n",
    "A = 0\n",
    "B = 1\n",
    "UNKNOWN = -1\n",
    "\n",
    "\"\"\"make_subsequences: Gets the contents of all .txt files from a folder and join them to a long string\n",
    "--------------- Paras: --------------------------------------------------------------------------\n",
    "@long_sequence:(str) The input text which is a long string joined from multiple .txt files \n",
    "@label: (str) Label name \n",
    "@sequence_length: the length of sequences \n",
    "--------------- Returns:-------------------------------------------------------------------------\n",
    "@X,y: Matrixes for preparing datasets. \n",
    "-------------------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "def make_subsequences(long_sequence, label, sequence_length = SEQ_LEN):\n",
    "    len_seq = len(long_sequence)\n",
    "    X = np.zeros(((len_seq - sequence_length)+1, sequence_length)) # Why not \"/\" but \"-\" \n",
    "    y = np.zeros((X.shape[0],1)) # This is for label <kept the X rows - X.shape[0]>\n",
    "    for i in range(X.shape[0]): \n",
    "        X[i] = long_sequence[i:i+sequence_length] # put the sequence to X matrix\n",
    "        y[i] = label # label the sequence\n",
    "    return X,y\n",
    "\n",
    "# Use the Tokenizer class from Keras to convert the long texts into a sequence of characters ? (looks like numbers) (not words)\n",
    "tokenizer = Tokenizer(char_level= True)\n",
    "# Make sure to fit all characters in texts from both authors\n",
    "tokenizer.fit_on_texts(All_Text[\"AuthorA\"] + All_Text[\"AuthorB\"])\n",
    "\n",
    "authorA_long_sequence = tokenizer.texts_to_sequences([All_Text[\"AuthorA\"]])[0]\n",
    "authorB_long_sequence = tokenizer.texts_to_sequences([All_Text[\"AuthorB\"]])[0]\n",
    "\n",
    "# Convert the long sequencese into sequence and label pairs\n",
    "X_authorA, y_authorA = make_subsequences(authorA_long_sequence, A)\n",
    "X_authorB, y_authorB = make_subsequences(authorB_long_sequence, B)\n",
    "\n",
    "# Print sizes of available data\n",
    "print(\"Number of characters: {}\".format(len(tokenizer.word_index)))\n",
    "print('author A sequences: {}'.format(X_authorA.shape))\n",
    "print('author B sequences: {}'.format(X_authorB.shape))\n",
    "\n",
    "# Calculate the number of unique words in the text \n",
    "# Compare the number of raw characters to the number of labeled sequences for each author\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts([All_Text[\"AuthorA\"], All_Text[\"AuthorB\"]])\n",
    "print(\"Total word count: \", len((All_Text[\"AuthorA\"] + ' ' + All_Text[\"AuthorB\"]).split(' ')))\n",
    "print (\"Total number of unique words: \", len(word_tokenizer.word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "spacious \n      Hai          \n kkkk    \n Third   NOwWW  \n"
    },
    {
     "data": {
      "text/plain": "'spacious hai kkkk third nowww'"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How we can join and split word after removing all the whitespaces and newlines \n",
    "string = \"         spacious \\n      Hai          \\n kkkk    \\n Third   NOwWW  \"\n",
    "print(string)\n",
    "\" \".join(string.lower().replace(\"\\n\",'').replace(\"Hai\",'').split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X_train shape: (357870, 30)\ny_train shape: (357870, 1)\nX_validate shape: (89468, 30)\ny_validate shape: (89468, 1)\n"
    }
   ],
   "source": [
    "###################### STEP 3: proceed to create our train, validation sets.##################################\n",
    "# 1.Stack x data together and y data together\n",
    "# 2.use train_test_split to split the dataset into 80% training and 20% validation\n",
    "# 3.Reshape the data to make sure that they are sequences of correct length\n",
    "\n",
    "#1. Stacking\n",
    "X = np.vstack((X_authorA, X_authorB))\n",
    "y = np.vstack((y_authorA, y_authorB))\n",
    "\n",
    "#2. Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y, train_size =0.8)\n",
    "\n",
    "#3. Ensure that the data is the same size as expected (batch_size, sequence length) (optional)\n",
    "# -1, the value is inferred from the length of the array and remaining dimension. (10,10,2) = (5,5,8) = (5,5,-1)\n",
    "X_train = X_train.reshape(-1, SEQ_LEN) \n",
    "X_val = X_val.reshape(-1, SEQ_LEN)\n",
    "\n",
    "# Print the shapes of the train, validation and test sets\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"X_validate shape: {}\".format(X_val.shape))\n",
    "print(\"y_validate shape: {}\".format(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, 30, 100)           5300      \n_________________________________________________________________\nsimple_rnn_4 (SimpleRNN)     (None, 156)               40092     \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 157       \n=================================================================\nTotal params: 45,549\nTrainable params: 45,549\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 357870 samples, validate on 89468 samples\nEpoch 1/20\n357870/357870 [==============================] - 45s 126us/step - loss: 0.6869 - accuracy: 0.5449 - val_loss: 0.6837 - val_accuracy: 0.5552\nEpoch 2/20\n357870/357870 [==============================] - 41s 114us/step - loss: 0.6829 - accuracy: 0.5567 - val_loss: 0.6862 - val_accuracy: 0.5489\nEpoch 3/20\n357870/357870 [==============================] - 40s 112us/step - loss: 0.6813 - accuracy: 0.5583 - val_loss: 0.6802 - val_accuracy: 0.5580\nEpoch 4/20\n357870/357870 [==============================] - 40s 112us/step - loss: 0.6742 - accuracy: 0.5723 - val_loss: 0.6664 - val_accuracy: 0.5891\nEpoch 5/20\n357870/357870 [==============================] - 45s 126us/step - loss: 0.6584 - accuracy: 0.6022 - val_loss: 0.6550 - val_accuracy: 0.6134\nEpoch 6/20\n357870/357870 [==============================] - 44s 123us/step - loss: 0.6392 - accuracy: 0.6289 - val_loss: 0.6258 - val_accuracy: 0.6448\nEpoch 7/20\n357870/357870 [==============================] - 52s 145us/step - loss: 0.6131 - accuracy: 0.6596 - val_loss: 0.6041 - val_accuracy: 0.6689\nEpoch 8/20\n357870/357870 [==============================] - 54s 151us/step - loss: 0.5817 - accuracy: 0.6901 - val_loss: 0.5694 - val_accuracy: 0.6991\nEpoch 9/20\n357870/357870 [==============================] - 51s 141us/step - loss: 0.5449 - accuracy: 0.7217 - val_loss: 0.5321 - val_accuracy: 0.7308\nEpoch 10/20\n357870/357870 [==============================] - 50s 140us/step - loss: 0.5063 - accuracy: 0.7504 - val_loss: 0.4984 - val_accuracy: 0.7534\nEpoch 11/20\n357870/357870 [==============================] - 50s 138us/step - loss: 0.4676 - accuracy: 0.7754 - val_loss: 0.4657 - val_accuracy: 0.7765\nEpoch 12/20\n357870/357870 [==============================] - 49s 138us/step - loss: 0.4336 - accuracy: 0.7969 - val_loss: 0.4450 - val_accuracy: 0.7887\nEpoch 13/20\n357870/357870 [==============================] - 49s 138us/step - loss: 0.4016 - accuracy: 0.8156 - val_loss: 0.4136 - val_accuracy: 0.8084\nEpoch 14/20\n357870/357870 [==============================] - 49s 138us/step - loss: 0.3758 - accuracy: 0.8300 - val_loss: 0.3962 - val_accuracy: 0.8194\nEpoch 15/20\n357870/357870 [==============================] - 49s 138us/step - loss: 0.3499 - accuracy: 0.8438 - val_loss: 0.3679 - val_accuracy: 0.8337\nEpoch 16/20\n357870/357870 [==============================] - 49s 138us/step - loss: 0.3263 - accuracy: 0.8566 - val_loss: 0.3561 - val_accuracy: 0.8405\nEpoch 17/20\n357870/357870 [==============================] - 50s 138us/step - loss: 0.3076 - accuracy: 0.8662 - val_loss: 0.3401 - val_accuracy: 0.8491\nEpoch 18/20\n357870/357870 [==============================] - 50s 140us/step - loss: 0.2870 - accuracy: 0.8767 - val_loss: 0.3210 - val_accuracy: 0.8578\nEpoch 19/20\n357870/357870 [==============================] - 50s 138us/step - loss: 0.2753 - accuracy: 0.8825 - val_loss: 0.3048 - val_accuracy: 0.8654\nEpoch 20/20\n357870/357870 [==============================] - 49s 138us/step - loss: 0.2583 - accuracy: 0.8905 - val_loss: 0.3126 - val_accuracy: 0.8665\nSaved model RNN to disk\n"
    }
   ],
   "source": [
    "###################### STEP 4: Buid and Train RNN, then save RNN model ##################################\n",
    "from keras.layers import SimpleRNN, Embedding, Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adadelta, Adam\n",
    "Embedding_size = 100\n",
    "RNN_size = 156\n",
    "Batch_size = 4096\n",
    "Epochs = 20\n",
    "\n",
    "RNN = Sequential()\n",
    "RNN.add(Embedding(len(tokenizer.word_index) + 1,Embedding_size, input_length = SEQ_LEN))\n",
    "RNN.add(SimpleRNN(RNN_size, return_sequences = False))\n",
    "RNN.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "RNN.compile(optimizer= 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "RNN.summary()\n",
    "\n",
    "RNN.fit(X_train, y_train, batch_size= Batch_size, epochs= Epochs, validation_data=(X_val,y_val))\n",
    "# Evaluate model \n",
    "scores = RNN.evaluate(X_val,y_val, verbose= 0)\n",
    "RNN.save(\"RNN.h5\")\n",
    "print(\"Saved model RNN to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Paper 1 is predicted to have been written by Author B, 12293 to 8484\nPaper 2 is predicted to have been written by Author B, 11384 to 8265\nPaper 3 is predicted to have been written by Author A, 6753 to 6634\nPaper 4 is predicted to have been written by Author A, 5109 to 4667\nPaper 5 is predicted to have been written by Author A, 6808 to 4949\n"
    }
   ],
   "source": [
    "#######################STEP 5:Applying the model to Unknown papers ###################################\n",
    "for paper in  sorted(os.listdir('./papers/Unknown/')):\n",
    "    unknown = preprocess_text('Unknown','./papers/Unknown/' + paper)\n",
    "    unknown_long_sequences = tokenizer.texts_to_sequences([unknown])[0]\n",
    "    X_sequences, _ = make_subsequences(unknown_long_sequences, UNKNOWN)\n",
    "    X_sequences = X_sequences.reshape((-1,SEQ_LEN))\n",
    "\n",
    "    votes_A = 0\n",
    "    votes_B = 0\n",
    "\n",
    "    y = RNN.predict(X_sequences)\n",
    "    # >0.5, convert the sequence of 0 and 1\n",
    "    y = y>0.5\n",
    "    votes_A = np.sum(y==0)\n",
    "    votes_B = np.sum(y==1)\n",
    "\n",
    "    print(\"Paper {} is predicted to have been written by {}, {} to {}\"\n",
    "                                            .format(paper.replace('paper_','').replace('.txt',''),\n",
    "                                            (\"Author A\" if votes_A > votes_B else \"Author B\"),\n",
    "                                            max(votes_A,votes_B),\n",
    "                                            min(votes_A,votes_B)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, 30, 100)           5300      \n_________________________________________________________________\nsimple_rnn_4 (SimpleRNN)     (None, 156)               40092     \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 157       \n=================================================================\nTotal params: 45,549\nTrainable params: 45,549\nNon-trainable params: 0\n_________________________________________________________________\naccuracy: 86.65%\n[Loaded_RNN] Paper 1 is predicted to have been written by Author B, 12293 to 8484\n[Loaded_RNN] Paper 2 is predicted to have been written by Author B, 11384 to 8265\n[Loaded_RNN] Paper 3 is predicted to have been written by Author A, 6753 to 6634\n[Loaded_RNN] Paper 4 is predicted to have been written by Author A, 5109 to 4667\n[Loaded_RNN] Paper 5 is predicted to have been written by Author A, 6808 to 4949\n"
    }
   ],
   "source": [
    "#######################STEP 6:load and evaluate a saved model ###################################\n",
    "from keras.models import load_model\n",
    "# load model\n",
    "Loaded_RNN = load_model('RNN.h5')\n",
    "# summarize model.\n",
    "Loaded_RNN.summary()\n",
    "score = Loaded_RNN.evaluate(X_val, y_val, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (Loaded_RNN.metrics_names[1], score[1]*100))\n",
    "\n",
    " ## Applying the loaded model to Unknown papers \n",
    "for paper in  sorted(os.listdir('./papers/Unknown/')):\n",
    "    unknown = preprocess_text('Unknown','./papers/Unknown/' + paper)\n",
    "    unknown_long_sequences = tokenizer.texts_to_sequences([unknown])[0]\n",
    "    X_sequences, _ = make_subsequences(unknown_long_sequences, UNKNOWN)\n",
    "    X_sequences = X_sequences.reshape((-1,SEQ_LEN))\n",
    "\n",
    "    votes_A = 0\n",
    "    votes_B = 0\n",
    "\n",
    "    y = Loaded_RNN.predict(X_sequences)\n",
    "    # >0.5, convert the sequence of 0 and 1\n",
    "    y = y>0.5\n",
    "    votes_A = np.sum(y==0)\n",
    "    votes_B = np.sum(y==1)\n",
    "\n",
    "    print(\"[Loaded_RNN] Paper {} is predicted to have been written by {}, {} to {}\"\n",
    "                                            .format(paper.replace('paper_','').replace('.txt',''),\n",
    "                                            (\"Author A\" if votes_A > votes_B else \"Author B\"),\n",
    "                                            max(votes_A,votes_B),\n",
    "                                            min(votes_A,votes_B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('virtual_env': venv)",
   "language": "python",
   "name": "python37464bitvirtualenvvenv9acec58b124a4af58980f597288afade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}