{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nUsing TensorFlow backend.\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 500)               28500     \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 501       \n=================================================================\nTotal params: 29,001\nTrainable params: 29,001\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 20 samples, validate on 20 samples\nEpoch 1/15\n20/20 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.4000 - val_loss: 0.6901 - val_accuracy: 0.5500\nEpoch 2/15\n20/20 [==============================] - 0s 150us/step - loss: 0.6407 - accuracy: 0.8000 - val_loss: 0.6507 - val_accuracy: 0.6500\nEpoch 3/15\n20/20 [==============================] - 0s 198us/step - loss: 0.5942 - accuracy: 1.0000 - val_loss: 0.6152 - val_accuracy: 0.7500\nEpoch 4/15\n20/20 [==============================] - 0s 99us/step - loss: 0.5529 - accuracy: 0.9500 - val_loss: 0.5836 - val_accuracy: 0.9000\nEpoch 5/15\n20/20 [==============================] - 0s 199us/step - loss: 0.5163 - accuracy: 0.9500 - val_loss: 0.5553 - val_accuracy: 0.9000\nEpoch 6/15\n20/20 [==============================] - 0s 149us/step - loss: 0.4838 - accuracy: 0.9500 - val_loss: 0.5302 - val_accuracy: 0.9000\nEpoch 7/15\n20/20 [==============================] - 0s 199us/step - loss: 0.4550 - accuracy: 0.9500 - val_loss: 0.5079 - val_accuracy: 0.9000\nEpoch 8/15\n20/20 [==============================] - 0s 150us/step - loss: 0.4292 - accuracy: 0.9500 - val_loss: 0.4880 - val_accuracy: 0.9000\nEpoch 9/15\n20/20 [==============================] - 0s 150us/step - loss: 0.4062 - accuracy: 0.9500 - val_loss: 0.4703 - val_accuracy: 0.9000\nEpoch 10/15\n20/20 [==============================] - 0s 150us/step - loss: 0.3855 - accuracy: 0.9500 - val_loss: 0.4546 - val_accuracy: 0.9000\nEpoch 11/15\n20/20 [==============================] - 0s 149us/step - loss: 0.3667 - accuracy: 0.9500 - val_loss: 0.4408 - val_accuracy: 0.9000\nEpoch 12/15\n20/20 [==============================] - 0s 200us/step - loss: 0.3497 - accuracy: 0.9500 - val_loss: 0.4284 - val_accuracy: 0.9000\nEpoch 13/15\n20/20 [==============================] - 0s 199us/step - loss: 0.3340 - accuracy: 0.9500 - val_loss: 0.4175 - val_accuracy: 0.9000\nEpoch 14/15\n20/20 [==============================] - 0s 148us/step - loss: 0.3196 - accuracy: 0.9500 - val_loss: 0.4077 - val_accuracy: 0.9000\nEpoch 15/15\n20/20 [==============================] - 0s 100us/step - loss: 0.3063 - accuracy: 0.9500 - val_loss: 0.3989 - val_accuracy: 0.9000\n10/10 [==============================] - 0s 100us/step\nAccuracy: 0.8999999761581421\n"
    }
   ],
   "source": [
    "# Exercise 17: Creating a neural network.\n",
    "#1. Open jupyter notebook\n",
    "#2. Import pandas so we can store the data in dataframe (df)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./Exercise17/train_comment_small_50.csv',sep=',')\n",
    "#3. Import regular expressions package \n",
    "import re\n",
    "#4. Create a func to preproces the reviews by removing HTML tags, escaped quotes and normal quotes\n",
    "def clean_comment(text):\n",
    "    #Strip HTML tags\n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    "    #Strip escaped quotes\n",
    "    text = text.replace('\\\\\"', '')\n",
    "    # text = re.sub('//\"', '', text)\n",
    "    #Strip quotes\n",
    "    text = text.replace('\"', '')\n",
    "    # text = re.sub('\"', '', text)\n",
    "    return text  \n",
    "\n",
    "#5. Apply this function to the reviews currently in the dataframe \n",
    "# Apply function clean_comment to column comment_text\n",
    "df['clean_comment'] = df['comment_text'].apply(clean_comment)\n",
    "\n",
    "#  https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html\n",
    "# View data with: df ; df[0:5]; df.head() ; df.tail(3) ; df.index ; df.columns ; df.to_numpy() etc.\n",
    "#  Slicing rows:  df.iloc[1:3, :]\n",
    "# pieces = [df[0:1], df[2:4]]\n",
    "# pd.concat(pieces)\n",
    "\n",
    "#6. Import train_test_split from scikit-learn to divide this data into Training set and a Validation set (Train on 0.8, Test on 0.2, X: text comment, y: label)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_comment'],df['toxic'], test_size=0.2)\n",
    "\n",
    "#7. Import nltk and stopwords from nltk lib\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "#8. ML or DL need numerical data as input, currently our data is text form. Thus, we ll use CountVectorizer to convert the words present in reviews into word count vectors \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True, stop_words=stopwords.words('english'),\n",
    "                            lowercase=True, min_df=3, max_df=0.9, max_features = 5000)\n",
    "\n",
    "X_train_onehot = vectorizer.fit_transform(X_train)\n",
    "# print(\"vectorizer.get_feature_names() is \",\"\\n\",len(vectorizer.get_feature_names()))\n",
    "\n",
    "#9. Now create 2 Layers Neural Net. (2 layers not include the input layer => \n",
    "# 2 layers Neral Network[1 input layer, 1 hidden layer, 1 output layer])\n",
    "#10. Import the model and layers from Keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "#11. Initiate the neural network \n",
    "NN = Sequential()\n",
    "\n",
    "#12. Add the hidden layer. Specify number of nodes the layer will have the activa\n",
    "NN.add(Dense(units= 500, activation='relu', input_dim=len(vectorizer.get_feature_names())))\n",
    "#13. Add the output layer. \n",
    "NN.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "#14. Compile the NN, decide which loss function, optimazation algorithm and performance metric we want to use. \n",
    "NN.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#15. Summarize our model\n",
    "NN.summary()\n",
    "\n",
    "#16. Train the model (train on the first 80, validate with the last 20)\n",
    "NN.fit(X_train_onehot[:-20], y_train[:-20], epochs = 15, batch_size = 128, verbose = 1, validation_data = (X_train_onehot[-20:],y_train[-20:]))\n",
    "#DONE Training now READY for Testing\n",
    "\n",
    "#17. Transform Input Validation data (test) into word count vector, and evaluate the NN.\n",
    "scores = NN.evaluate(vectorizer.transform(X_test), y_test,verbose=1)\n",
    "print(\"Accuracy:\",scores[1])\n",
    "\n",
    "# save your model \n",
    "NN.save('NN.hd5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('virtual_env': venv)",
   "language": "python",
   "name": "python37464bitvirtualenvvenv9acec58b124a4af58980f597288afade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}