{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\nUsing TensorFlow backend.\nModel: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1 (Dense)              (None, 500)               59500     \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 501       \n=================================================================\nTotal params: 60,001\nTrainable params: 60,001\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 40 samples, validate on 39 samples\nEpoch 1/20\n40/40 [==============================] - 0s 2ms/step - loss: 0.7526 - accuracy: 0.2500 - val_loss: 0.7007 - val_accuracy: 0.4615\nEpoch 2/20\n40/40 [==============================] - 0s 125us/step - loss: 0.7023 - accuracy: 0.4500 - val_loss: 0.6636 - val_accuracy: 0.5641\nEpoch 3/20\n40/40 [==============================] - 0s 100us/step - loss: 0.6632 - accuracy: 0.7750 - val_loss: 0.6280 - val_accuracy: 0.7436\nEpoch 4/20\n40/40 [==============================] - 0s 99us/step - loss: 0.6254 - accuracy: 0.8500 - val_loss: 0.5948 - val_accuracy: 0.8718\nEpoch 5/20\n40/40 [==============================] - 0s 125us/step - loss: 0.5901 - accuracy: 0.8500 - val_loss: 0.5641 - val_accuracy: 0.8718\nEpoch 6/20\n40/40 [==============================] - 0s 101us/step - loss: 0.5575 - accuracy: 0.9000 - val_loss: 0.5353 - val_accuracy: 0.8974\nEpoch 7/20\n40/40 [==============================] - 0s 125us/step - loss: 0.5269 - accuracy: 0.9000 - val_loss: 0.5100 - val_accuracy: 0.8974\nEpoch 8/20\n40/40 [==============================] - 0s 150us/step - loss: 0.5000 - accuracy: 0.9000 - val_loss: 0.4871 - val_accuracy: 0.8974\nEpoch 9/20\n40/40 [==============================] - 0s 99us/step - loss: 0.4754 - accuracy: 0.9000 - val_loss: 0.4663 - val_accuracy: 0.8974\nEpoch 10/20\n40/40 [==============================] - 0s 100us/step - loss: 0.4528 - accuracy: 0.9000 - val_loss: 0.4478 - val_accuracy: 0.8974\nEpoch 11/20\n40/40 [==============================] - 0s 125us/step - loss: 0.4321 - accuracy: 0.9000 - val_loss: 0.4312 - val_accuracy: 0.8974\nEpoch 12/20\n40/40 [==============================] - 0s 100us/step - loss: 0.4131 - accuracy: 0.9000 - val_loss: 0.4163 - val_accuracy: 0.8974\nEpoch 13/20\n40/40 [==============================] - 0s 124us/step - loss: 0.3954 - accuracy: 0.9000 - val_loss: 0.4029 - val_accuracy: 0.8974\nEpoch 14/20\n40/40 [==============================] - 0s 125us/step - loss: 0.3790 - accuracy: 0.9000 - val_loss: 0.3909 - val_accuracy: 0.8974\nEpoch 15/20\n40/40 [==============================] - 0s 100us/step - loss: 0.3637 - accuracy: 0.9000 - val_loss: 0.3803 - val_accuracy: 0.8974\nEpoch 16/20\n40/40 [==============================] - 0s 124us/step - loss: 0.3493 - accuracy: 0.9000 - val_loss: 0.3707 - val_accuracy: 0.8974\nEpoch 17/20\n40/40 [==============================] - 0s 125us/step - loss: 0.3356 - accuracy: 0.9000 - val_loss: 0.3621 - val_accuracy: 0.8974\nEpoch 18/20\n40/40 [==============================] - 0s 124us/step - loss: 0.3226 - accuracy: 0.9000 - val_loss: 0.3544 - val_accuracy: 0.8974\nEpoch 19/20\n40/40 [==============================] - 0s 149us/step - loss: 0.3102 - accuracy: 0.9500 - val_loss: 0.3475 - val_accuracy: 0.8974\nEpoch 20/20\n40/40 [==============================] - 0s 100us/step - loss: 0.2983 - accuracy: 0.9750 - val_loss: 0.3413 - val_accuracy: 0.8974\n20/20 [==============================] - 0s 50us/step\nAccuracy: 1.0\nscores: [0.28996342420578003, 1.0]\n"
    }
   ],
   "source": [
    "#1. Import pandas so we can store the data in dataframe (df)\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./Exercise17/train_comment_small_100.csv',sep=',')\n",
    "#2. Import regular expressions package\n",
    "import re\n",
    "#3.Create a func to preproces the reviews (clean and prepare data)\n",
    "def clean_comment(text):\n",
    "    #Strip HTML tags\n",
    "    text = re.sub('<[^<]+?>', ' ', text)\n",
    "    #Strip escaped quotes\n",
    "    text = text.replace('\\\\\"', '')\n",
    "    # text = re.sub('//\"', '', text)\n",
    "    #Strip quotes\n",
    "    text = text.replace('\"', '')\n",
    "    # text = re.sub('\"', '', text)\n",
    "    return text  \n",
    "# remove NaN\n",
    "df = df.dropna()\n",
    "df['cleaned_cmts'] = df['comment_text'].apply(clean_comment)\n",
    "\n",
    "# count = 0\n",
    "# for index in df['comment_text']:\n",
    "#     clean_comment(df['comment_text'][count])\n",
    "#     print(\"/n\",\"index \",count)\n",
    "#     count=count+1\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_cmts'],df['toxic'], test_size=0.2)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "vectorizer = CountVectorizer(binary = True, stop_words=stopwords.words('english'),\n",
    "  \n",
    "                            lowercase=True, min_df=3, max_df=0.9, max_features = 5000)   \n",
    "# Only fit on the training data. \n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Import the model and layers from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "#Initialize Neural Network \n",
    "Neural_Net = Sequential()\n",
    "# Add the hidden layer. \n",
    "Neural_Net.add(Dense(units= 500, activation='relu', input_dim=len(vectorizer.get_feature_names())))\n",
    "# Add the output layer. \n",
    "Neural_Net.add(Dense(units=1,activation='sigmoid'))\n",
    "#Compile the NN, decide which loss function, optimazation algorithm and performance metric we want to use. \n",
    "Neural_Net.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#Summarize our model\n",
    "Neural_Net.summary()\n",
    "\n",
    "#Train the model \n",
    "Neural_Net.fit(X_train_vec[:-39],y_train[:-39],epochs=20, batch_size= 128, verbose=1, validation_data=(X_train_vec[-39:],y_train[-39:]))\n",
    "\n",
    "#Evaluate the model \n",
    "scores = Neural_Net.evaluate(vectorizer.transform(X_test),y_test,verbose=1)\n",
    "print(\"Accuracy:\",scores[1])\n",
    "print(\"scores:\",scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('virtual_env': venv)",
   "language": "python",
   "name": "python37464bitvirtualenvvenv9acec58b124a4af58980f597288afade"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}