{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package tagsets to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package tagsets is already up-to-date!\n"
    },
    {
     "data": {
      "text/plain": "[('I', 'PRP'),\n ('enjoy', 'VBP'),\n ('playing', 'VBG'),\n ('sports', 'NNS'),\n ('like', 'IN'),\n ('badminton', 'NN')]"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercis 10: Performing Rule-Based POS Tagging\n",
    "# 1: import nltk and punkt\n",
    "# NLTK has trainied POS tagger \n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "# 2: Store an input string in a variable named s\n",
    "s = 'I enjoy playing sports like badminton'\n",
    "# 3:Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(s)\n",
    "# 4: Apply the POS tagger on the tokens and then print the tagset\n",
    "tags = nltk.pos_tag(tokens)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "PRP: pronoun, personal\n    hers herself him himself hisself it itself me myself one oneself ours\n    ourselves ownself self she thee theirs them themselves they thou thy us\nVBP: verb, present tense, not 3rd person singular\n    predominate wrap resort sue twist spill cure lengthen brush terminate\n    appear tend stray glisten obtain comprise detest tease attract\n    emphasize mold postpone sever return wag ...\nVBG: verb, present participle or gerund\n    telegraphing stirring focusing angering judging stalling lactating\n    hankerin' alleging veering capping approaching traveling besieging\n    encrypting interrupting erasing wincing ...\nNN: noun, common, singular or mass\n    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n    investment slide humour falloff slick wind hyena override subhumanity\n    machinist ...\n"
    }
   ],
   "source": [
    "# 5: To understand \"NN\" POS tag stands for\n",
    "nltk.help.upenn_tagset(\"PRP\")\n",
    "nltk.help.upenn_tagset(\"VBP\")\n",
    "nltk.help.upenn_tagset(\"VBG\")\n",
    "nltk.help.upenn_tagset(\"NN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[('but', 'CC'),\n ('I', 'PRP'),\n ('mentioned', 'VBD'),\n ('that', 'IN'),\n ('Im', 'NNP'),\n ('going', 'VBG'),\n ('to', 'TO'),\n ('play', 'VB'),\n ('fun', 'JJ'),\n ('badmiton', 'NN'),\n ('for', 'IN'),\n ('the', 'DT'),\n ('play', 'NN'),\n ('on', 'IN'),\n ('this', 'DT'),\n ('Wed', 'NNP')]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a sentence with hommonyms and see it tagset\n",
    "sentence2 = 'but I mentioned that Im going to play fun badmiton for the play on this Wed'\n",
    "tag2 = nltk.pos_tag(nltk.word_tokenize(sentence2))\n",
    "tag2\n",
    "# This show that POS taggers can differentiate between homonyms like Play(VerB) and play(NouN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "but ---> CCONJ ---> CC ---> conjunction, coordinating\nI ---> PRON ---> PRP ---> pronoun, personal\nmentioned ---> VERB ---> VBD ---> verb, past tense\nthat ---> SCONJ ---> IN ---> conjunction, subordinating or preposition\nI ---> PRON ---> PRP ---> pronoun, personal\nam ---> AUX ---> VBP ---> verb, non-3rd person singular present\ngoing ---> VERB ---> VBG ---> verb, gerund or present participle\nto ---> PART ---> TO ---> infinitival \"to\"\nplay ---> VERB ---> VB ---> verb, base form\nbadmiton ---> NOUN ---> NN ---> noun, singular or mass\nfor ---> ADP ---> IN ---> conjunction, subordinating or preposition\nthe ---> DET ---> DT ---> determiner\nplay ---> NOUN ---> NN ---> noun, singular or mass\non ---> ADP ---> IN ---> conjunction, subordinating or preposition\nthis ---> DET ---> DT ---> determiner\nWednesday ---> PROPN ---> NNP ---> noun, proper singular\n"
    }
   ],
   "source": [
    "# Exercise 11: Performing Stochastic POS tagging. spacy's POS tagger is a stochastic one.\n",
    "import spacy\n",
    "# load spaCy7's en\\core_web_sm model for English \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Fit the model on the sentence we want to assign POS tags to. \n",
    "doc = nlp(u\"but I mentioned that I am going to play badmiton for the play on this Wednesday\")\n",
    "# Tokenize the sentence, assign POS tags and print them\n",
    "    # print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "    #         token.shape_, token.is_alpha, token.is_stop)\n",
    "for token in doc: \n",
    "    print(token.text, \"--->\", token.pos_, \"--->\", token.tag_, \"--->\", spacy.explain(token.tag_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'verb, modal auxiliary'"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to understand the tag ,\"VBD\",\"NN\",\"PRP\"\n",
    "spacy.explain(\"VBZ\")\n",
    "spacy.explain(\"MD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package tagsets to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package tagsets is already up-to-date!\n"
    }
   ],
   "source": [
    "# FROM Exercis 10: Performing Rule-Based POS Tagging\n",
    "# 1: import nltk and punkt\n",
    "# NLTK has trainied POS tagger \n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "# 2: Store an input string in a variable named s\n",
    "s = 'I enjoy playing some fun sports like badminton'\n",
    "\n",
    "# 3:Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(s)\n",
    "# 4: Apply the POS tagger on the tokens and then print the tagset\n",
    "tagset = nltk.pos_tag(tokens)\n",
    "tagset\n",
    "\n",
    "# Exercise 12: perform Chunking with NLTK \n",
    "# 1 create a regular epression that will search for a noun phrase\n",
    "rule = r\"\"\"Noun Phrase: {(<DT>?<JJ>*<NN>?<NN>)|(<DT>?<JJ>*<NNS>)|<NN>}\"\"\"\n",
    "# look for <DT> determiner (ex: the) or <JJ> Adjective(ex: fun), then a single Noun\n",
    "# 2.Create an instance o RegxpParser and feed it the rule\n",
    "chunkParser = nltk.RegexpParser(rule)\n",
    "#3. Give chunkParser the tagset containing the tokens with their respective POS tags so that it can \n",
    "# perform chunking , and then draw the chunks:\n",
    "# chunked = chunkParser.parse(tagset)\n",
    "# chunked.draw()\n",
    "#4. different sentence\n",
    "a = \"The beautiful lady jumped on the sport car and he drove away into the high way\"\n",
    "tagset2 = nltk.pos_tag(nltk.word_tokenize(a))\n",
    "chunkParser2 = chunkParser.parse(tagset2)\n",
    "chunkParser2.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The beautiful lady lady nsubj\nthe sport car car pobj\nhe he nsubj\nthe high way way pobj\n"
    }
   ],
   "source": [
    "#Exercise 13: Perform Chunking with spaCy (spaCy is Supposed to be easier than NLTK)\n",
    "import spacy\n",
    "# load spaCy7's en\\core_web_sm model for English \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Fit the model on the sentence we want to assign POS tags to. \n",
    "doc = nlp(u\"The beautiful lady jumped on the sport car and he drove away into the high way\")\n",
    "#  Apply noun_chunks on this model, and for each chunk, print the text of the chunk\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_)\n",
    "# for token in doc.noun_chunks:\n",
    "#     # print(token.text)\n",
    "#     print(token.root.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package tagsets to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package tagsets is already up-to-date!\n"
    }
   ],
   "source": [
    "import nltk \n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')\n",
    "s2 = \"The beautiful lady jumped on the sport car and he drove away into the high way\"\n",
    "chink_tagset = nltk.pos_tag(nltk.word_tokenize(s2))\n",
    "\n",
    "# Exercise 14. Performing Chinking \n",
    "#1. Create rule that chunks the entire corpus and only creates chinks out of the words or phrases tagged as nouns or noun phrases:\n",
    "rule = r\"\"\"Chink: {<.*>+}\n",
    "}<VB.?|CC|RB|JJ|IN|DT|T0>+{\"\"\"\n",
    "# This regular expression is telling machine to ignore all words that are not Nouns or noun phrases(only extract N, N phrase as a chink)\n",
    "\n",
    "#2. Create an instance of RegexpParser and feed it the rule\n",
    "chinkParser = nltk.RegexpParser(rule)\n",
    "#3. Give ChinkParser the tagset and perform chinking \n",
    "chinked = chinkParser.parse(chink_tagset)\n",
    "chinked.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[nltk_data] Downloading package treebank to\n[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package treebank is already up-to-date!\n[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\nTagged sentences:  3914\nTagged words: 100676\nTraining completed\nAccuracy: 0.8933392254539612\n"
    }
   ],
   "source": [
    "# Activity 2: Build and Train POS Tagger* (Can I train my own tagger for CODE ?)\n",
    "\n",
    "#1. Pick a corpus to train (using nltk treebank corpus)\n",
    "import nltk \n",
    "import re\n",
    "import pprint \n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "nltk.download('treebank')\n",
    "tagged_sentences = nltk.corpus.treebank.tagged_sents()\n",
    "print(tagged_sentences[0])\n",
    "print(\"Tagged sentences: \", len(tagged_sentences))\n",
    "print(\"Tagged words:\", len(nltk.corpus.treebank.tagged_words()))\n",
    "# [(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.')]\n",
    "# Tagged sentences:  3914\n",
    "# Tagged words: 100676\n",
    "\n",
    "#2. Determine what are the feature the TAGGER will consder to assgin a tag to a word\n",
    "\"\"\"Before starting training a classifier, we must agree first on what features to use. Most obvious choices are: the word itself, the word before and the word after. That’s a good start, but we can do so much better. For example, the 2-letter suffix is a great indicator of past-tense verbs, ending in “-ed”. 3-letter suffix helps recognize the present participle ending in “-ing” \"\"\"\n",
    "def features(sentence, index):\n",
    "    \"\"\" sentence: [w1, w2, ...], index: the index of the word \"\"\"\n",
    "    return  {\n",
    "        'word': sentence[index],\n",
    "        'is_first': index == 0,\n",
    "        'is_last': index == len(sentence) - 1,\n",
    "        'is_capitalized': sentence[index][0].upper() == sentence[index][0],\n",
    "        'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "        'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "        'prefix-1': sentence[index][0],\n",
    "        'prefix-2': sentence[index][:2],\n",
    "        'prefix-3': sentence[index][:3],\n",
    "        'suffix-1': sentence[index][-1],\n",
    "        'suffix-2': sentence[index][-2:],\n",
    "        'suffix-3': sentence[index][-3:],\n",
    "        'prev_word': '' if index == 0 else sentence[index - 1],\n",
    "        'next_word': '' if index == len(sentence) - 1 else sentence[index + 1],\n",
    "        'has_hyphen': '-' in sentence[index],\n",
    "        'is_numeric': sentence[index].isdigit(),\n",
    "        'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "    }\n",
    "\n",
    "# pprint.pprint(features(['This', 'is', 'a', 'sentence'], 2))\n",
    "# {'capitals_inside': False,\n",
    "#  'has_hyphen': False,\n",
    "#  'is_all_caps': False,\n",
    "#  'is_all_lower': True,\n",
    "#  'is_capitalized': False,\n",
    "#  'is_first': False,\n",
    "#  'is_last': False,\n",
    "#  'is_numeric': False,\n",
    "#  'next_word': 'sentence',\n",
    "#  'prefix-1': 'a',\n",
    "#  'prefix-2': 'a',\n",
    "#  'prefix-3': 'a',\n",
    "#  'prev_word': 'is',\n",
    "#  'suffix-1': 'a',\n",
    "#  'suffix-2': 'a',\n",
    "#  'suffix-3': 'a',\n",
    "#  'word': 'a'}\n",
    "\n",
    "#3. Create a fucntion to strip the tagged words of their tags, then feed them to the TAGGER\n",
    "\"\"\"Remove the tag for each tagged term.\n",
    "    :param tagged_sentence: a POS tagged sentence\n",
    "    :type tagged_sentence: list\n",
    "    :return: a list of tags\n",
    "    :rtype: list of strings\"\"\"\n",
    "def untag(tagged_sentence):\n",
    "    return [w for w, t in tagged_sentence] #?team\n",
    "\n",
    "# S_test = ((u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.'),(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.'))\n",
    "\n",
    "# untag(S_test)\n",
    "\n",
    "# #4. Build a dataset and split into Training and Test data sets, Assign features(X), POS tags(Y)\n",
    "# \"\"\" Split the dataset for training and testing\"\"\"\n",
    "cutoff = int(.75 * len(tagged_sentences))\n",
    "training_sentences = tagged_sentences[:cutoff]\n",
    "test_sentences = tagged_sentences[cutoff:]\n",
    "\n",
    "# print(len(training_sentences))\n",
    "# print(len(test_sentences))\n",
    "\n",
    "def TransformToDataSet(tagged_sentences):\n",
    "    X, y = [], []\n",
    " \n",
    "    for tagged in tagged_sentences:\n",
    "        for index in range(len(tagged)):\n",
    "            X.append(features(untag(tagged), index))\n",
    "            y.append(tagged[index][1])\n",
    " \n",
    "    return X, y\n",
    "\n",
    "# # Get X,Y \n",
    "X_train, y_train = TransformToDataSet(training_sentences)\n",
    "\n",
    "#5. Use Decision Tree classifier to train the tagger. \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "clf = Pipeline([\n",
    "    ('vectorizer', DictVectorizer(sparse=False)),\n",
    "    ('classifier', DecisionTreeClassifier(criterion='entropy'))\n",
    "])\n",
    " \n",
    "clf.fit(X_train[:10000], y_train[:10000])   # Use only the first 10K samples if you're running it multiple times. It takes a fair bit :)\n",
    " \n",
    "print('Training completed')\n",
    " \n",
    "X_test, Y_test = TransformToDataSet(test_sentences)\n",
    " \n",
    "print(\"Accuracy:\", clf.score(X_test, Y_test))\n",
    "# Accuracy expectation: 0.904186083882\n",
    "\n",
    "#6. Import the classifier, initialize it, fit the model on training and print the score. \n",
    "# def pos_tag(sentence):\n",
    "#     tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "#     print(sentence, tags)\n",
    "#     return zip(sentence, tags)\n",
    "\n",
    "# print(pos_tag(word_tokenize('This is my friend, John.')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[\"This', 'DT\", \"is', 'VBZ\", \"my', 'NN\", \"friend', 'NN\", \",', ',\", \"John', 'NNP\", \".', '.\"]\n[['This', 'DT'], ['is', 'VBZ'], ['my', 'NN'], ['friend', 'NN'], [',', ','], ['John', 'NNP'], ['.', '.']]\n[('This', 'DT'), ('is', 'VBZ'), ('my', 'NN'), ('friend', 'NN'), (',', ','), ('John', 'NNP'), ('.', '.')]\n"
    },
    {
     "data": {
      "text/plain": "[('This', 'DT'),\n ('is', 'VBZ'),\n ('my', 'NN'),\n ('friend', 'NN'),\n (',', ','),\n ('John', 'NNP'),\n ('.', '.')]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#6. Import the classifier, initialize it, fit the model on training and print the score. \n",
    "def pos_tag(sentence):\n",
    "    tags = clf.predict([features(sentence, index) for index in range(len(sentence))])\n",
    "    NewSentence = [\"', '\".join(item) for item in zip(sentence, tags)]   # 1st way\n",
    "    tagged_sentence  = list(map(list, zip(sentence, tags)))             # 2nd way \n",
    "    PosTagSentence = list(zip(sentence, tags))\n",
    "\n",
    "    print(NewSentence)\n",
    "    print(tagged_sentence)\n",
    "    print(PosTagSentence)\n",
    "\n",
    "    return PosTagSentence\n",
    " \n",
    "pos_tag(word_tokenize('This is my friend, John.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}